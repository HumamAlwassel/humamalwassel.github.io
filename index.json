[{"authors":["admin"],"categories":null,"content":"I am a PhD student at King Abdullah University of Science and Technology (KAUST) in Saudi Arabia, currently focused on the development of novel computer vision techniques for video understanding. I am part of the diverse Image and Video Understanding Lab (IVUL) advised by Bernard Ghanem. I received my MSc degree in Computer Science from KAUST under Bernard Ghanem, and my undergraduate degree in both Computer Science and Mathematics from Cornell University. My main research interests are in video understanding and human activity detection. In general, I am interested in computer vision and machine learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://humamalwassel.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a PhD student at King Abdullah University of Science and Technology (KAUST) in Saudi Arabia, currently focused on the development of novel computer vision techniques for video understanding. I am part of the diverse Image and Video Understanding Lab (IVUL) advised by Bernard Ghanem. I received my MSc degree in Computer Science from KAUST under Bernard Ghanem, and my undergraduate degree in both Computer Science and Mathematics from Cornell University.","tags":null,"title":"Humam Alwassel","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"https://humamalwassel.github.io/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":["**Humam Alwassel**","Dhruv Mahajan","Lorenzo Torresani","Bernard Ghanem","Du Tran"],"categories":null,"content":" BibTex @misc{alwassel2019selfsupervised, title={Self-Supervised Learning by Cross-Modal Audio-Video Clustering}, author={Humam Alwassel and Dhruv Mahajan and Lorenzo Torresani and Bernard Ghanem and Du Tran}, year={2019}, eprint={1911.12667}, archivePrefix={arXiv}, primaryClass={cs.CV} }  XDC Visualization Gallery  Click on the images below to view in full resolution and see the captions.       Overview of our framework. We present Single-Modality Deep Clustering (SDC) baseline vs. our three different proposed models: Multi-Head Deep Clustering (MDC), Concatenation Deep Clustering (CDC), and Cross-Modal Deep Clustering (XDC) for multi-modal deep clustering. Unlabeled videos are inputted into the video and audio encoders (E_v and E_a) to produce visual and audio features (f_v and f_a). These features, or the concatenation of them, are clustered using k-means. The cluster assignments are then used as pseudo-labels to train the two encoders. The process is started with randomly-initialized encoders, then alternates between clustering to generate pseudo-labels and training to improve the encoders. The four models employ different ways to cluster features and generate self-supervision signals for learning the visual and audio representations.\n      Visualization of XDC clusters on Kinetics videos. We present the top-2 audio clusters (left) and the top-2 video clusters (right) in terms of purity with respect to the original Kinetics labels. For each cluster, we show a single frame of the 10 closest videos to the cluster centroid. Interestingly, our self-supervised XDC model learned to group videos of \u0026#39;\u0026#39;scuba diving\u0026#39;\u0026#39; with \u0026#39;\u0026#39;snorkeling\u0026#39;\u0026#39; (second left, cluster #105) based on their audio features and those of \u0026#39;\u0026#39;scuba diving\u0026#39;\u0026#39; and \u0026#39;\u0026#39;feeding fish\u0026#39;\u0026#39; (rightmost, cluster #27) based on their visual features.\n      XDC audio clusters. Top and bottom 10 XDC audio clusters ranked by clustering purity w.r.t. Kinetics labels. Each row presents a cluster where we show the center frame of the 10 clips that are nearest to the cluster center.\n      XDC video clusters. Top and bottom 10 XDC video clusters ranked by clustering purity w.r.t. Kinetics labels. Each row presents a cluster where we show the center frame of the 10 clips that are nearest to the cluster center.\n      R(2\u0026#43;1)D filters learned with self-supervised XDC vs. fully-supervised training. (a) R(2\u0026#43;1)D \u0026#39;\u0026#39;conv_1\u0026#39;\u0026#39; filters learned by fully-supervised training on Kinetics. (b) The same filters learned by self-supervised XDC pretraining on IG65M. XDC learns a more diverse set of temporal filters compared to fully-supervised pretraining.\n      Audio cluster 000\n      Audio cluster 001\n      Audio cluster 002\n      Audio cluster 003\n      Audio cluster 004\n      Audio cluster 005\n      Audio cluster 006\n      Audio cluster 007\n      Audio cluster 008\n      Audio cluster 009\n      Audio cluster 010\n      Audio cluster 011\n      Audio cluster 012\n      Audio cluster 013\n      Audio cluster 014\n      Audio cluster 015\n      Audio cluster 016\n      Audio cluster 017\n      Audio cluster 018\n      Audio cluster 019\n      Audio cluster 020\n      Audio cluster 021\n      Audio cluster 022\n      Audio cluster 023\n      Audio cluster 024\n      Audio cluster 025\n      Audio cluster 026\n      Audio cluster 027\n      Audio cluster 028\n      Audio cluster 029\n      Audio cluster 030\n      Audio cluster 031\n      Audio cluster 032\n      Audio cluster 033\n      Audio cluster 034\n      Audio cluster 035\n      Audio cluster 036\n      Audio cluster 037\n      Audio cluster 038\n      Audio cluster 039\n      Audio cluster 040\n      Audio cluster 041\n      Audio cluster 042\n      Audio cluster 043\n      Audio cluster 044\n      Audio cluster 045\n      Audio cluster 046\n      Audio cluster 047\n      Audio cluster 048\n      Audio cluster 049\n      Audio cluster 050\n      Audio cluster 051\n      Audio cluster 052\n      Audio cluster 053\n      Audio cluster 054\n      Audio cluster 055\n      Audio cluster 056\n      Audio cluster 057\n      Audio cluster 058\n      Audio cluster 059\n      Audio cluster 060\n      Audio cluster 061\n      Audio cluster 062\n      Audio cluster 063\n      Audio cluster 064\n      Audio cluster 065\n      Audio cluster 066\n      Audio cluster 067\n      Audio cluster 068\n      Audio cluster 069\n      Audio cluster 070\n      Audio cluster 071\n      Audio cluster 072\n      Audio cluster 073\n      Audio cluster 074\n      Audio cluster 075\n      Audio cluster 076\n      Audio cluster 077\n      Audio cluster 078\n      Audio cluster 079\n      Audio cluster 080\n      Audio cluster 081\n      Audio cluster 082\n      Audio cluster 083\n      Audio cluster 084\n      Audio cluster 085\n      Audio cluster 086\n      Audio cluster 087\n      Audio cluster 088\n      Audio cluster 089\n      Audio cluster 090\n      Audio cluster 091\n      Audio cluster 092\n      Audio cluster 093\n      Audio cluster 094\n      Audio cluster 095\n      Audio cluster 096\n      Audio cluster 097\n      Audio cluster 098\n      Audio cluster 099\n      Audio cluster 100\n      Audio cluster 101\n      Audio cluster 102\n      Audio cluster 103\n      Audio cluster 104\n      Audio cluster 105\n      Audio cluster 106\n      Audio cluster 107\n      Audio cluster 108\n      Audio cluster 109\n      Audio cluster 110\n      Audio cluster 111\n      Audio cluster 112\n      Audio cluster 113\n      Audio cluster 114\n      Audio cluster 115\n      Audio cluster 116\n      Audio cluster 117\n      Audio cluster 118\n      Audio cluster 119\n      Audio cluster 120\n      Audio cluster 121\n      Audio cluster 122\n      Audio cluster 123\n      Audio cluster 124\n      Audio cluster 125\n      Audio cluster 126\n      Audio cluster 127\n      Visual cluster 000\n      Visual cluster 001\n      Visual cluster 002\n      Visual cluster 003\n      Visual cluster 004\n      Visual cluster 005\n      Visual cluster 006\n      Visual cluster 007\n      Visual cluster 008\n      Visual cluster 009\n      Visual cluster 010\n      Visual cluster 011\n      Visual cluster 012\n      Visual cluster 013\n      Visual cluster 014\n      Visual cluster 015\n      Visual cluster 016\n      Visual cluster 017\n      Visual cluster 018\n      Visual cluster 019\n      Visual cluster 020\n      Visual cluster 021\n      Visual cluster 022\n      Visual cluster 023\n      Visual cluster 024\n      Visual cluster 025\n      Visual cluster 026\n      Visual cluster 027\n      Visual cluster 028\n      Visual cluster 029\n      Visual cluster 030\n      Visual cluster 031\n      Visual cluster 032\n      Visual cluster 033\n      Visual cluster 034\n      Visual cluster 035\n      Visual cluster 036\n      Visual cluster 037\n      Visual cluster 038\n      Visual cluster 039\n      Visual cluster 040\n      Visual cluster 041\n      Visual cluster 042\n      Visual cluster 043\n      Visual cluster 044\n      Visual cluster 045\n      Visual cluster 046\n      Visual cluster 047\n      Visual cluster 048\n      Visual cluster 049\n      Visual cluster 050\n      Visual cluster 051\n      Visual cluster 052\n      Visual cluster 053\n      Visual cluster 054\n      Visual cluster 055\n      Visual cluster 056\n      Visual cluster 057\n      Visual cluster 058\n      Visual cluster 059\n      Visual cluster 060\n      Visual cluster 061\n      Visual cluster 062\n      Visual cluster 063\n         ","date":1575244800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575244800,"objectID":"26f437e74deaf02f928ba3a3d7948c51","permalink":"https://humamalwassel.github.io/publication/xdc/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/xdc/","section":"publication","summary":"The visual and audio modalities are highly correlated yet they contain different information. Their strong correlation makes it possible to predict the semantics of one from the other with good accuracy. Their intrinsic differences make cross-modal prediction a potentially more rewarding pretext task for self-supervised learning of video and audio representations compared to within-modality learning. Based on this intuition, we propose *Cross-Modal Deep Clustering (XDC)*, a novel self-supervised method that leverages unsupervised clustering in one modality (*e.g.* audio) as a supervisory signal for the other modality (*e.g.* video). This cross-modal supervision helps XDC utilize the semantic correlation and the differences between the two modalities. Our experiments show that XDC significantly outperforms single-modality clustering and other multi-modal variants. Our XDC achieves state-of-the-art accuracy among self-supervised methods on several video and audio benchmarks including HMDB51, UCF101, ESC50, and DCASE. Most importantly, the video model pretrained with XDC significantly outperforms the same model pretrained with full-supervision on both ImageNet and Kinetics in action recognition on HMDB51 and UCF101. To the best of our knowledge, XDC is the first method to demonstrate that self-supervision outperforms large-scale full-supervision in representation learning for action recognition.","tags":[],"title":"Self-Supervised Learning by Cross-Modal Audio-Video Clustering","type":"publication"},{"authors":["**Humam Alwassel**","Alejandro Pardo","Fabian Caba Heilbron","Ali Thabet","Bernard Ghanem"],"categories":null,"content":" BibTex @misc{alwassel2019refineloc, title={RefineLoc: Iterative Refinement for Weakly-Supervised Action Localization}, author={Humam Alwassel and Alejandro Pardo and Fabian Caba Heilbron and Ali Thabet and Bernard Ghanem}, year={2019}, eprint={1904.00227}, archivePrefix={arXiv}, primaryClass={cs.CV} }  ","date":1564012800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564012800,"objectID":"46cc743fc5f12eacf57e218ec653e2ac","permalink":"https://humamalwassel.github.io/publication/refineloc/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/refineloc/","section":"publication","summary":"Video action detectors are usually trained using datasets with fully-supervised temporal annotations. Building such datasets is an expensive task. To alleviate this problem, recent methods have tried to leverage weak labelling, where videos are untrimmed and only a video-level label is available. In this paper, we propose RefineLoc, a new weakly-supervised temporal action localization method. RefineLoc uses an iterative refinement approach by estimating and training on snippet-level pseudo ground truth at every iteration. We show the benefit of this iterative approach and present an extensive analysis of different pseudo ground truth generators. We show the effectiveness of our model on two standard action datasets, ActivityNet v1.2 and THUMOS14. RefineLoc equipped with a segment prediction-based pseudo ground truth generator improves the state-of-the-art in weakly-supervised temporal localization on the challenging and large-scale ActivityNet dataset by 1.5% in average mAP.","tags":[],"title":"RefineLoc: Iterative Refinement for Weakly-Supervised Action Localization","type":"publication"},{"authors":["Ali Thabet","**Humam Alwassel**","Bernard Ghanem"],"categories":null,"content":" BibTex @misc{thabet2019mortonnet, title={MortonNet: Self-Supervised Learning of Local Features in 3D Point Clouds}, author={Ali Thabet and Humam Alwassel and Bernard Ghanem}, year={2019}, eprint={1904.00230}, archivePrefix={arXiv}, primaryClass={cs.CV} }  ","date":1553904000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553904000,"objectID":"a7e425af1387f641e84f13650ea4d34d","permalink":"https://humamalwassel.github.io/publication/mortonnet/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/mortonnet/","section":"publication","summary":"We present a self-supervised task on point clouds, in order to learn meaningful point-wise features that encode local structure around each point. Our self-supervised network, named MortonNet, operates directly on unstructured/unordered point clouds. Using a multi-layer RNN, MortonNet predicts the next point in a point sequence created by a popular and fast Space Filling Curve, the Morton-order curve. The final RNN state (coined Morton feature) is versatile and can be used in generic 3D tasks on point clouds. In fact, we show how Morton features can be used to significantly improve performance (+3% for 2 popular semantic segmentation algorithms) in the task of semantic segmentation of point clouds on the challenging and large-scale S3DIS dataset. We also show how MortonNet trained on S3DIS transfers well to another large-scale dataset, vKITTI, leading to an improvement over state-of-the-art of 13.8%. Finally, we use Morton features to train a much simpler and more stable model for part segmentation in ShapeNet. Our results show how our self-supervised task results in features that are useful for 3D segmentation tasks, and generalize well to other datasets.","tags":[],"title":"MortonNet: Self-Supervised Learning of Local Features in 3D Point Clouds","type":"publication"},{"authors":[],"categories":[],"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://humamalwassel.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["**Humam Alwassel**","Fabian Caba Heilbron","Bernard Ghanem"],"categories":null,"content":" BibTex @inproceedings{alwassel_2018_actionsearch, title={Action Search: Spotting Targets in Videos and Its Application to Temporal Action Localization}, author={Alwassel, Humam and Caba Heilbron, Fabian and Ghanem, Bernard}, booktitle={The European Conference on Computer Vision (ECCV)}, month={September} year={2018} }  ","date":1532476800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1532476800,"objectID":"12dfebc1b6772a3060778691add0d527","permalink":"https://humamalwassel.github.io/publication/action-search/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/action-search/","section":"publication","summary":"State-of-the-art temporal action detectors inefficiently search the entire video for specific actions. Despite the encouraging progress these methods achieve, it is crucial to design automated approaches that only explore parts of the video which are the most relevant to the actions being searched for. To address this need, we propose the new problem of *action spotting* in video, which we define as finding a specific action in a video while observing a small portion of that video. Inspired by the observation that humans are extremely efficient and accurate in spotting and finding action instances in video, we propose *Action Search*, a novel Recurrent Neural Network approach that mimics the way humans spot actions. Moreover, to address the absence of data recording the behavior of human annotators, we put forward the *Human Searches* dataset, which compiles the search sequences employed by human annotators spotting actions in the AVA and THUMOS14 datasets. We consider temporal action localization as an application of the action spotting problem. Experiments on the THUMOS14 dataset reveal that our model is not only able to explore the video efficiently (observing on average **17.3%** of the video) but it also accurately finds human activities with **30.8%** mAP.","tags":[],"title":"Action Search: Spotting Targets in Videos and Its Application to Temporal Action Localization","type":"publication"},{"authors":["**Humam Alwassel**","Fabian Caba Heilbron","Victor Escorcia","Bernard Ghanem"],"categories":null,"content":" BibTex @inproceedings{alwassel_2018_detad, title={Diagnosing Error in Temporal Action Detectors}, author={Alwassel, Humam and Caba Heilbron, Fabian and Escorcia, Victor and Ghanem, Bernard}, booktitle={The European Conference on Computer Vision (ECCV)}, month={September} year={2018} }  ","date":1532476800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1532476800,"objectID":"1f29d1ab1afd42fd186ef61f74587dda","permalink":"https://humamalwassel.github.io/publication/detad/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/detad/","section":"publication","summary":"Despite the recent progress in video understanding and the continuous rate of improvement in temporal action localization throughout the years, it is still unclear how far (or close?) we are to solving the problem. To this end, we introduce a new diagnostic tool to analyze the performance of temporal action detectors in videos and compare different methods beyond a single scalar metric. We exemplify the use of our tool by analyzing the performance of the top rewarded entries in the latest ActivityNet action localization challenge. Our analysis shows that the most impactful areas to work on are: strategies to better handle temporal context around the instances, improving the robustness w.r.t. the instance absolute and relative size, and strategies to reduce the localization errors. Moreover, our experimental analysis finds the lack of agreement among annotator is not a major roadblock to attain progress in the field. Our diagnostic tool is publicly available to keep fueling the minds of other researchers with additional insights about their algorithms.","tags":[],"title":"DETAD: Diagnosing Error in Temporal Action Detectors","type":"publication"}]