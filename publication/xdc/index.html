<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.6.2">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Humam Alwassel">

  
  
  
    
  
  <meta name="description" content="The visual and audio modalities are highly correlated yet they contain different information. Their strong correlation makes it possible to predict the semantics of one from the other with good accuracy. Their intrinsic differences make cross-modal prediction a potentially more rewarding pretext task for self-supervised learning of video and audio representations compared to within-modality learning. Based on this intuition, we propose *Cross-Modal Deep Clustering (XDC)*, a novel self-supervised method that leverages unsupervised clustering in one modality (*e.g.* audio) as a supervisory signal for the other modality (*e.g.* video). This cross-modal supervision helps XDC utilize the semantic correlation and the differences between the two modalities. Our experiments show that XDC significantly outperforms single-modality clustering and other multi-modal variants. Our XDC achieves state-of-the-art accuracy among self-supervised methods on several video and audio benchmarks including HMDB51, UCF101, ESC50, and DCASE. Most importantly, the video model pretrained with XDC significantly outperforms the same model pretrained with full-supervision on both ImageNet and Kinetics in action recognition on HMDB51 and UCF101. To the best of our knowledge, XDC is the first method to demonstrate that self-supervision outperforms large-scale full-supervision in representation learning for action recognition.">

  
  <link rel="alternate" hreflang="en-us" href="https://humamalwassel.github.io/publication/xdc/">

  


  
  
  
  <meta name="theme-color" content="#328cc1">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
    

    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  





<script async src="https://www.googletagmanager.com/gtag/js?id=UA-111353817-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           document.location = url;
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target);  
  }

  gtag('js', new Date());
  gtag('config', 'UA-111353817-1', {});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon-32.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://humamalwassel.github.io/publication/xdc/">

  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Humam Alwassel">
  <meta property="og:url" content="https://humamalwassel.github.io/publication/xdc/">
  <meta property="og:title" content="Self-Supervised Learning by Cross-Modal Audio-Video Clustering | Humam Alwassel">
  <meta property="og:description" content="The visual and audio modalities are highly correlated yet they contain different information. Their strong correlation makes it possible to predict the semantics of one from the other with good accuracy. Their intrinsic differences make cross-modal prediction a potentially more rewarding pretext task for self-supervised learning of video and audio representations compared to within-modality learning. Based on this intuition, we propose *Cross-Modal Deep Clustering (XDC)*, a novel self-supervised method that leverages unsupervised clustering in one modality (*e.g.* audio) as a supervisory signal for the other modality (*e.g.* video). This cross-modal supervision helps XDC utilize the semantic correlation and the differences between the two modalities. Our experiments show that XDC significantly outperforms single-modality clustering and other multi-modal variants. Our XDC achieves state-of-the-art accuracy among self-supervised methods on several video and audio benchmarks including HMDB51, UCF101, ESC50, and DCASE. Most importantly, the video model pretrained with XDC significantly outperforms the same model pretrained with full-supervision on both ImageNet and Kinetics in action recognition on HMDB51 and UCF101. To the best of our knowledge, XDC is the first method to demonstrate that self-supervision outperforms large-scale full-supervision in representation learning for action recognition."><meta property="og:image" content="https://humamalwassel.github.io/publication/xdc/featured.jpg">
  <meta property="twitter:image" content="https://humamalwassel.github.io/publication/xdc/featured.jpg"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2017-01-01T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2019-11-25T00:00:00&#43;00:00">
  

  


    











<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Article",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://humamalwassel.github.io/publication/xdc/"
  },
  "headline": "Self-Supervised Learning by Cross-Modal Audio-Video Clustering",
  
  "image": [
    "https://humamalwassel.github.io/publication/xdc/featured.jpg"
  ],
  
  "datePublished": "2017-01-01T00:00:00Z",
  "dateModified": "2019-11-25T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "**Humam Alwassel**"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Humam Alwassel",
    "logo": {
      "@type": "ImageObject",
      "url": "https://humamalwassel.github.io/img/icon-512.png"
    }
  },
  "description": "The visual and audio modalities are highly correlated yet they contain different information. Their strong correlation makes it possible to predict the semantics of one from the other with good accuracy. Their intrinsic differences make cross-modal prediction a potentially more rewarding pretext task for self-supervised learning of video and audio representations compared to within-modality learning. Based on this intuition, we propose *Cross-Modal Deep Clustering (XDC)*, a novel self-supervised method that leverages unsupervised clustering in one modality (*e.g.* audio) as a supervisory signal for the other modality (*e.g.* video). This cross-modal supervision helps XDC utilize the semantic correlation and the differences between the two modalities. Our experiments show that XDC significantly outperforms single-modality clustering and other multi-modal variants. Our XDC achieves state-of-the-art accuracy among self-supervised methods on several video and audio benchmarks including HMDB51, UCF101, ESC50, and DCASE. Most importantly, the video model pretrained with XDC significantly outperforms the same model pretrained with full-supervision on both ImageNet and Kinetics in action recognition on HMDB51 and UCF101. To the best of our knowledge, XDC is the first method to demonstrate that self-supervision outperforms large-scale full-supervision in representation learning for action recognition."
}
</script>

  

  


  


  





  <title>Self-Supervised Learning by Cross-Modal Audio-Video Clustering | Humam Alwassel</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    
    
      <a class="navbar-brand" href="/">Humam Alwassel</a>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#news"><span>News</span></a>
        </li>

        
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Publications</span><span class="caret"></span>
          </a>
          <div class="dropdown-menu">
            
              <a class="dropdown-item" href="/#featured"><span>Featured</span></a>
            
              <a class="dropdown-item" href="/#publications"><span>All</span></a>
            
          </div>
        </li>

        
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Experience</span><span class="caret"></span>
          </a>
          <div class="dropdown-menu">
            
              <a class="dropdown-item" href="/#professional_experience"><span>Professional Experience</span></a>
            
              <a class="dropdown-item" href="/#administrative_experience"><span>Administrative Experience</span></a>
            
          </div>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#awards"><span>Awards</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      <li class="nav-item">
        <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
      </li>
      

      

    </ul>

  </div>
</nav>


  <div class="pub">

  




















  
  


<div class="article-container pt-3">
  <h1>Self-Supervised Learning by Cross-Modal Audio-Video Clustering</h1>

  

  


<div class="article-metadata">

  
  
  
  
  <div>
    



  
  <span><a href="/authors/humam-alwassel/"><strong>Humam Alwassel</strong></a></span>, <span><a href="/authors/dhruv-mahajan/">Dhruv Mahajan</a></span>, <span><a href="/authors/lorenzo-torresani/">Lorenzo Torresani</a></span>, <span><a href="/authors/bernard-ghanem/">Bernard Ghanem</a></span>, <span><a href="/authors/du-tran/">Du Tran</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    November 2019
  </span>
  

  

  

  
  
  

  
  

</div>

  











  



<div class="btn-links mb-3">
  
  








  





<button type="button" class="btn btn-outline-primary my-1 mr-1 js-cite-modal"
        data-filename="/publication/xdc/cite.bib">
  Cite
</button>














  
  
  
    
  
  
  
  
  
    
    
      
    
  
  <a class="btn btn-outline-primary my-1 mr-1" href="/" >
    
    Preprint (coming soon)
  </a>

  
  
  
    
  
  
  
  
  
    
    
      
    
  
  <a class="btn btn-outline-primary my-1 mr-1" href="/" >
    
    Code (coming soon)
  </a>


</div>


</div>


<div class="article-header container-fluid featured-image-wrapper mt-4 mb-4" style="max-width: 2460px; max-height: 990px;">
  <div style="position: relative">
    <img src="/publication/xdc/featured.jpg" alt="" class="featured-image">
    <span class="article-header-caption">Cross-Modal Deep Clustering (XDC) is a novel self-supervised method that leverages unsupervised clustering in one modality (e.g. audio) as a supervisory signal for the other modality (e.g. video).</span>
  </div>
</div>



  <div class="article-container">

    
    <h3>Abstract</h3>
    <p class="pub-abstract">The visual and audio modalities are highly correlated yet they contain different information. Their strong correlation makes it possible to predict the semantics of one from the other with good accuracy. Their intrinsic differences make cross-modal prediction a potentially more rewarding pretext task for self-supervised learning of video and audio representations compared to within-modality learning. Based on this intuition, we propose <em>Cross-Modal Deep Clustering (XDC)</em>, a novel self-supervised method that leverages unsupervised clustering in one modality (<em>e.g.</em> audio) as a supervisory signal for the other modality (<em>e.g.</em> video). This cross-modal supervision helps XDC utilize the semantic correlation and the differences between the two modalities. Our experiments show that XDC significantly outperforms single-modality clustering and other multi-modal variants. Our XDC achieves state-of-the-art accuracy among self-supervised methods on several video and audio benchmarks including HMDB51, UCF101, ESC50, and DCASE. Most importantly, the video model pretrained with XDC significantly outperforms the same model pretrained with full-supervision on both ImageNet and Kinetics in action recognition on HMDB51 and UCF101. To the best of our knowledge, XDC is the first method to demonstrate that self-supervision outperforms large-scale full-supervision in representation learning for action recognition.</p>
    

    
    <div class="row">
      <div class="col-md-1"></div>
      <div class="col-md-10">
        <div class="row">
          <div class="col-12 col-md-3 pub-row-heading">Type</div>
          <div class="col-12 col-md-9">
            
            
            <a href="/publication/#3">
              Preprint
            </a>
            
          </div>
        </div>
      </div>
      <div class="col-md-1"></div>
    </div>
    <div class="d-md-none space-below"></div>
    

    
    <div class="row">
      <div class="col-md-1"></div>
      <div class="col-md-10">
        <div class="row">
          <div class="col-12 col-md-3 pub-row-heading">Publication</div>
          <div class="col-12 col-md-9">On <em>arXiv</em></div>
        </div>
      </div>
      <div class="col-md-1"></div>
    </div>
    <div class="d-md-none space-below"></div>
    

    <div class="space-below"></div>

    <div class="article-style">

<h3 id="bibtex-coming-soon">BibTex (coming soon)</h3>

<pre><code></code></pre>

<h1 id="xdc-visualization-gallery">XDC Visualization Gallery</h1>

<div class="alert alert-note">
  <div>
    Click on the images below to view in full resolution and see the captions.
  </div>
</div>









  
  


<div class="gallery">

  
  
  
  
    
    
    
    
    
      
        
      
    
  <a data-fancybox="gallery-gallery" href="/publication/xdc/gallery/0_xdc_pipeline_figure.png" data-caption="&lt;strong&gt;Overview of our framework.&lt;/strong&gt; We present Single-Modality Deep Clustering (SDC) baseline vs. our three different proposed models: Multi-Head Deep Clustering (MDC), Concatenation Deep Clustering (CDC), and Cross-Modal Deep Clustering (XDC) for multi-modal deep clustering. Unlabeled videos are inputted into the video and audio encoders (&lt;em&gt;E_v&lt;/em&gt; and &lt;em&gt;E_a&lt;/em&gt;) to produce visual and audio features (&lt;em&gt;f_v&lt;/em&gt; and &lt;em&gt;f_a&lt;/em&gt;). These features, or the concatenation of them, are clustered using &lt;em&gt;k&lt;/em&gt;-means. The cluster assignments are then used as pseudo-labels to train the two encoders. The process is started with randomly-initialized encoders, then alternates between clustering to generate pseudo-labels and training to improve the encoders. The four models employ different ways to cluster features and generate self-supervision signals for learning the visual and audio representations.">
  <img src="/publication/xdc/gallery/0_xdc_pipeline_figure_huea2f144d01d04a705ca74fdcc27b1352_445228_0x190_resize_lanczos_2.png" alt="">
  </a>
  
    
    
    
    
    
      
        
      
    
  <a data-fancybox="gallery-gallery" href="/publication/xdc/gallery/1_xdc_top_2_visual_and_audio_clusters.png" data-caption="&lt;strong&gt;Visualization of XDC clusters on Kinetics videos&lt;/strong&gt;. We present the top-2 audio clusters (left) and the top-2 video clusters (right) in terms of purity with respect to the original Kinetics labels. For each cluster, we show a single frame of the 10 closest videos to the cluster centroid. Interestingly, our self-supervised XDC model learned to group videos of &lt;code&gt;scuba diving&lt;/code&gt; with &lt;code&gt;snorkeling&lt;/code&gt; (second left, cluster #105) based on their audio features and those of &lt;code&gt;scuba diving&lt;/code&gt; and &lt;code&gt;feeding fish&lt;/code&gt; (rightmost, cluster #27) based on their visual features.">
  <img src="/publication/xdc/gallery/1_xdc_top_2_visual_and_audio_clusters_hue087a574602fb413ff3819f76dd55da9_609603_0x190_resize_lanczos_2.png" alt="">
  </a>
  
    
    
    
    
    
      
        
      
    
  <a data-fancybox="gallery-gallery" href="/publication/xdc/gallery/2_xdc_audio_clusters_top_bottom_10.png" data-caption="&lt;strong&gt;XDC audio clusters.&lt;/strong&gt; Top and bottom 10 XDC audio clusters ranked by clustering purity w.r.t. Kinetics labels. Each row presents a cluster where we show the center frame of the 10 clips that are nearest to the cluster center.">
  <img src="/publication/xdc/gallery/2_xdc_audio_clusters_top_bottom_10_huc8bbd77e166e7afc67183fc1ed262197_2207529_0x190_resize_lanczos_2.png" alt="">
  </a>
  
    
    
    
    
    
      
        
      
    
  <a data-fancybox="gallery-gallery" href="/publication/xdc/gallery/3_xdc_visual_clusters_top_bottom_10.png" data-caption="&lt;strong&gt;XDC video clusters.&lt;/strong&gt; Top and bottom 10 XDC video clusters ranked by clustering purity w.r.t. Kinetics labels. Each row presents a cluster where we show the center frame of the 10 clips that are nearest to the cluster center.">
  <img src="/publication/xdc/gallery/3_xdc_visual_clusters_top_bottom_10_hu43a30e136d46fadb7e40a15ac067f35b_2253005_0x190_resize_lanczos_2.png" alt="">
  </a>
  
    
    
    
    
    
      
        
      
    
  <a data-fancybox="gallery-gallery" href="/publication/xdc/gallery/4_xdc_filters.png" data-caption="&lt;strong&gt;R(2&#43;1)D filters learned with self-supervised XDC vs. fully-supervised training.&lt;/strong&gt; (a) R(2&#43;1)D &lt;code&gt;conv_1&lt;/code&gt; filters learned by fully-supervised training on Kinetics. (b) The same filters learned by self-supervised XDC pretraining on IG65M. XDC learns a more diverse set of temporal filters compared to fully-supervised pretraining.">
  <img src="/publication/xdc/gallery/4_xdc_filters_hu96db66595d7ed781e43d10a9e23b53af_594106_0x190_resize_lanczos_2.png" alt="">
  </a>
  

  
</div>
</div>

    







<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://humamalwassel.github.io/publication/xdc/&amp;text=Self-Supervised%20Learning%20by%20Cross-Modal%20Audio-Video%20Clustering" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://humamalwassel.github.io/publication/xdc/&amp;t=Self-Supervised%20Learning%20by%20Cross-Modal%20Audio-Video%20Clustering" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Self-Supervised%20Learning%20by%20Cross-Modal%20Audio-Video%20Clustering&amp;body=https://humamalwassel.github.io/publication/xdc/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://humamalwassel.github.io/publication/xdc/&amp;title=Self-Supervised%20Learning%20by%20Cross-Modal%20Audio-Video%20Clustering" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Self-Supervised%20Learning%20by%20Cross-Modal%20Audio-Video%20Clustering%20https://humamalwassel.github.io/publication/xdc/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://humamalwassel.github.io/publication/xdc/&amp;title=Self-Supervised%20Learning%20by%20Cross-Modal%20Audio-Video%20Clustering" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>












  
  
    
  
  






  
  
  
  
  <div class="media author-card content-widget-hr">
    

    <div class="media-body">
      <h5 class="card-title"><a href="/authors/humam-alwassel/"></a></h5>
      
      
      <ul class="network-icon" aria-hidden="true">
  
</ul>

    </div>
  </div>









  
  



  </div>
</div>

      

    
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js" integrity="sha256-1zu+3BnLYV9LdiY85uXMzii3bdrkelyp37e0ZyTAQh0=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/r.min.js"></script>
        
      

      
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.bcfae8267aba63cc55af53a503896bd9.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    &copy; 2019 Humam Alwassel &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
